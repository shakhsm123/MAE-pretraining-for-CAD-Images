{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3632f267-0539-44a3-91ad-9589b8db1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "Gaussian smoothing with Pytorch.\n",
    "Source:\n",
    "https://discuss.pytorch.org/t/is-there-anyway-to-do-gaussian-filtering-for-an-image-2d-3d-in-pytorch/12351/10\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2, order=0, device=None):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        self.padd = kernel_size // 2\n",
    "        self.dim = dim\n",
    "        self.std = sigma\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "\n",
    "        if order == 'xx':\n",
    "            kernel[0, 0] = ((kernel[0, 0] * ((meshgrids[0].T - self.padd) ** 2 - self.std ** 2) / self.std ** 4))\n",
    "        elif order == 'yy':\n",
    "            kernel[0, 0] = ((kernel[0, 0] * ((meshgrids[0] - self.padd) ** 2 - self.std ** 2) / self.std ** 4))\n",
    "        elif order == 'xy':\n",
    "            kernel[0, 0] = kernel[0, 0] * (meshgrids[0] - self.padd) * (meshgrids[0].T - self.padd) / self.std ** 4\n",
    "\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "        kernel.requires_grad = False\n",
    "        if device is not None:\n",
    "            kernel = kernel.to(device)\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            x (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        if self.dim == 2:\n",
    "            x = F.pad(x, (self.padd, self.padd, self.padd, self.padd), mode='reflect')\n",
    "        return self.conv(x, weight=self.weight, groups=self.groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13feab1-b044-4a86-87f4-20864bf63968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class SoftFrangiFilter2D(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, sigmas, beta, device, c=2):\n",
    "        \"\"\"\n",
    "        Apply Soft Frangi filter on a 3d tensor.\n",
    "        Arguments:\n",
    "            channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "                have this number of channels as well.\n",
    "            kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "            sigmas (list, sequence): List of standard deviations of the gaussian kernels.\n",
    "            beta (float, sequence): Beta parameter of Frangi filter.\n",
    "            beta (ั, sequence): ะก parameter of Frangi filter.\n",
    "                Default value is 2 (spatial).\n",
    "        \"\"\"\n",
    "        super(SoftFrangiFilter2D, self).__init__()\n",
    "        self.sigmas = sigmas\n",
    "        self.gaus_filters = []\n",
    "        self.beta = beta\n",
    "        self.c = c\n",
    "        for sigma in sigmas:\n",
    "            self.gaus_filters.append([])\n",
    "            for order in ['xx', 'yy', 'xy']:\n",
    "                self.gaus_filters[-1].append(GaussianSmoothing(channels, 7, sigma, c, order, device))\n",
    "\n",
    "    def _calc_frangi_response(self, xx, yy, xy):\n",
    "        \"\"\"\n",
    "        Calculate Frangi filter response give the second order derivatives.\n",
    "        Arguments:\n",
    "            xx (torch.Tensor, sequence): (bs, channels, h, w), second order derivative on x-axis\n",
    "            yy (torch.Tensor, sequence): (bs, channels, h, w), second order derivative on y-axis\n",
    "            xy (torch.Tensor, sequence): (bs, channels, h, w), second order derivative on xy-axes\n",
    "        \"\"\"\n",
    "        lambda_t1 = ((xx + yy) + torch.sqrt((xx - yy) ** 2 + 4 * xy ** 2 + 1e-6)) / 2\n",
    "        lambda_t2 = ((xx + yy) - torch.sqrt((xx - yy) ** 2 + 4 * xy ** 2 + 1e-6)) / 2\n",
    "        lambdas = torch.stack((lambda_t1, lambda_t2), dim=0)\n",
    "        lambdas_abs, sorted_ind = torch.sort(torch.abs(lambdas), dim=0)\n",
    "        lambda2_sign = (lambdas * sorted_ind).sum(dim=0)\n",
    "        lambda1 = lambdas_abs[0]\n",
    "        lambda2 = lambdas_abs[1]\n",
    "        blobness = torch.zeros(lambda1.shape, device=lambda1.device)\n",
    "        blobness[lambda2 != 0] = blobness[lambda2 != 0] + torch.exp(\n",
    "            -(lambda1[lambda2 != 0] / lambda2[lambda2 != 0]) ** 2 / (2 * self.beta ** 2))\n",
    "        hess_struc = (1 - torch.exp(-torch.sqrt(lambda1 ** 2 + lambda2 ** 2) ** 2 / (2 * self.c ** 2)))\n",
    "        vness = blobness * hess_struc\n",
    "        vness[lambda2_sign > 0] = vness[lambda2_sign > 0] * 0\n",
    "        return vness, blobness, hess_struc, lambda1, lambda2\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Apply Soft Frangi filter on a batch of images.\n",
    "        Arguments:\n",
    "            img (torch.Tensor, sequence): Tensor of shape (bs, channels, h, w)\n",
    "        \"\"\"\n",
    "        frangi_resp = torch.zeros((len(self.sigmas),) + img.shape, dtype=torch.float32, device=img.device)\n",
    "        for i, _ in enumerate(self.sigmas):\n",
    "            xx = self.gaus_filters[i][0](img)\n",
    "            yy = self.gaus_filters[i][1](img)\n",
    "            xy = self.gaus_filters[i][2](img)\n",
    "            vness, _, _, _, _ = self._calc_frangi_response(xx, yy, xy)\n",
    "            frangi_resp[i] = vness\n",
    "        # max_frangi_resp = (torch.softmax(frangi_resp, dim=0) * frangi_resp).sum(dim=0)\n",
    "        return frangi_resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee869ea-05a2-46ac-baf9-f3b92e16da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "#from util.pos_embed import get_2d_sincos_pos_embed\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "#from util.soft_frangi.soft_frangi_filter2d import SoftFrangiFilter2D\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\"Masked Autoencoder with Vision Transformer backbone\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_norm=None, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_norm=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True)  # decoder to patch\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.frangi = SoftFrangiFilter2D(channels=1, sigmas=range(2,10,1), beta=0.5, c=2, device=\"cuda\")\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Initialize positional embeddings and patch embeddings\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # Initialize weights for patch embedding\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # Initialize weights for linear and layer norm layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"Patchify input images\"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p)\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3)\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Unpatchify input sequences\"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(x.shape[0], h, w, p, p, 3)\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(x.shape[0], 3, h * p, h * p)\n",
    "        return imgs\n",
    "    \n",
    "    def guided_masking(self, x, mask_ratio, sigma=0.05):\n",
    "        patch_size = self.patch_embed.patch_size[0]\n",
    "        N, L, D = x.shape\n",
    "        num_patches = int(np.sqrt(L))\n",
    "        noise = torch.zeros(N, L, device=x.device)\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        imgs = x.clone()\n",
    "        imgs = self.unpatchify(imgs).mean(dim=1, keepdim=True)\n",
    "\n",
    "        response = self.frangi(imgs)\n",
    "        response = torch.cat((torch.zeros_like(response[0:1]), response), dim=0)\n",
    "        response = torch.max(response, dim=0)[0]\n",
    "\n",
    "        patches = response.reshape((N, num_patches,patch_size,num_patches,patch_size))\n",
    "        patches = torch.einsum(\"nhawb->nhwab\", patches)\n",
    "        patches = patches.reshape((N,num_patches,num_patches,patch_size*patch_size))\n",
    "        patches = patches.mean(axis=-1)\n",
    "        patches += torch.randn_like(patches) * sigma\n",
    "        noise = patches.view(N, -1)\n",
    "\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        # x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return ids_keep, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio, sigma=0.05):\n",
    "        patches = self.patchify(x.clone())\n",
    "        ids_keep, mask, ids_restore = self.guided_masking(patches, mask_ratio, sigma=sigma)\n",
    "        # embed patches\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, x.shape[-1]))\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75, sigma=0.05):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio, sigma)\n",
    "        pred = self.forward_decoder(latent, ids_restore)\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "    \n",
    "    \n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# set recommended archs\n",
    "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b471bb67-1c32-423d-81bb-0a1935f6cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# --------------------------------------------------------\n",
    "# Position embedding utils\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    \n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df92593f-d51d-4456-bf37-7864ade75406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ImageNet normalization values\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    \"\"\"\n",
    "    Function to display an image. The image should be in [H, W, 3] format\n",
    "    and normalized in the range [0, 1].\n",
    "    \"\"\"\n",
    "    assert image.shape[2] == 3  # Ensure the image has 3 channels\n",
    "    # Denormalize the image from [0, 1] to ImageNet scale and display it\n",
    "    image = (image * imagenet_std + imagenet_mean) * 255\n",
    "    image = np.clip(image, 0, 255).astype(np.uint8)  # Clip values to valid image range\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def run_one_image(img, model, mask_ratio=0.75, sigma=0.05):\n",
    "    \"\"\"\n",
    "    Function to run a single image through the MAE model, and display:\n",
    "    - Original image\n",
    "    - Masked image\n",
    "    - Reconstructed image\n",
    "    - Reconstructed + visible patches\n",
    "    \"\"\"\n",
    "    # Convert image to a PyTorch tensor (from NumPy array)\n",
    "    img = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "    # Move the image tensor to CUDA (GPU) if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Make sure the model is also moved to CUDA\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Add batch dimension to image\n",
    "    x = img.unsqueeze(dim=0)  # Add batch dimension\n",
    "    x = torch.einsum('nhwc->nchw', x)  # Convert HWC to NCHW format\n",
    "\n",
    "    # Run the model (using guided masking and reconstruction)\n",
    "    loss, y, mask = model(x.float(), mask_ratio=mask_ratio, sigma=sigma)\n",
    "    y = model.unpatchify(y)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach()\n",
    "\n",
    "    # Ensure mask is on the same device as input and output\n",
    "    mask = mask.detach().to(device)\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)  # (N, H*W, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().to(device)\n",
    "\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "\n",
    "    # Masked image (visible patches replaced by 0)\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # Display the results\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0].cpu().numpy(), \"original\")\n",
    "\n",
    "    # Masked image\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0].cpu().numpy(), \"masked\")\n",
    "\n",
    "    # Reconstruction\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0].cpu().numpy(), \"reconstruction\")\n",
    "\n",
    "    # Reconstruction + visible patches\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0].cpu().numpy(), \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# img = (your image as a numpy array here, scaled to [0, 1])\n",
    "# model_mae = (your pretrained model here)\n",
    "# run_one_image(img, model_mae, mask_ratio=0.05, sigma=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56663fe7-4043-457d-9e4d-69f3dc00ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
    "    # Define model dictionary\n",
    "    model_dict = {\n",
    "        'mae_vit_base_patch16': mae_vit_base_patch16_dec512d8b,\n",
    "        'mae_vit_large_patch16': mae_vit_large_patch16_dec512d8b,\n",
    "        'mae_vit_huge_patch14': mae_vit_huge_patch14_dec512d8b,\n",
    "    }\n",
    "    \n",
    "    # Check if the arch exists in the dictionary\n",
    "    if arch not in model_dict:\n",
    "        raise ValueError(f\"Architecture '{arch}' is not defined in the model dictionary.\")\n",
    "    \n",
    "    # Build model\n",
    "    model = model_dict[arch]()\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cuda')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2de978-3450-4ad5-b6e1-41a0ded47425",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_mae\u001b[38;5;241m=\u001b[39m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mShakhnazar\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoint-80.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae_vit_base_patch16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mprepare_model\u001b[1;34m(chkpt_dir, arch)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchitecture \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not defined in the model dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43march\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load model checkpoint\u001b[39;00m\n\u001b[0;32m     17\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(chkpt_dir, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 206\u001b[0m, in \u001b[0;36mmae_vit_base_patch16_dec512d8b\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmae_vit_base_patch16_dec512d8b\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 206\u001b[0m     model \u001b[38;5;241m=\u001b[39m MaskedAutoencoderViT(\n\u001b[0;32m    207\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m    208\u001b[0m         decoder_embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, decoder_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, decoder_num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m    209\u001b[0m         mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, norm_layer\u001b[38;5;241m=\u001b[39mpartial(nn\u001b[38;5;241m.\u001b[39mLayerNorm, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.__init__\u001b[1;34m(self, img_size, patch_size, in_chans, embed_dim, depth, num_heads, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, norm_layer, norm_pix_loss)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_pred \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(decoder_embed_dim, patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m in_chans, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# decoder to patch\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pix_loss \u001b[38;5;241m=\u001b[39m norm_pix_loss\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrangi \u001b[38;5;241m=\u001b[39m \u001b[43mSoftFrangiFilter2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_weights()\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mSoftFrangiFilter2D.__init__\u001b[1;34m(self, channels, sigmas, beta, device, c)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaus_filters\u001b[38;5;241m.\u001b[39mappend([])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m order \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxy\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaus_filters[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mGaussianSmoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[9], line 68\u001b[0m, in \u001b[0;36mGaussianSmoothing.__init__\u001b[1;34m(self, channels, kernel_size, sigma, dim, order, device)\u001b[0m\n\u001b[0;32m     66\u001b[0m kernel\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;241m=\u001b[39m channels\n",
      "File \u001b[1;32m~\\anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model_mae=prepare_model(r'C:\\Users\\Shakhnazar\\Downloads\\checkpoint-80.pth', 'mae_vit_base_patch16')\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c3863-a762-4afe-976c-1ec6ad2135b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_mae.eval()\n",
    "\n",
    "# Load an example image\n",
    "image_path =r\"\"  # Replace with your image path\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV format) to RGB\n",
    "img = cv2.resize(img, (224, 224))  # Resize to match the model's input size\n",
    "img = img / 255.0  # Normalize image to range [0, 1]\n",
    "\n",
    "# Run the image through the model\n",
    "run_one_image(img, model_mae, mask_ratio=0.2, sigma=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac37c5-90f4-43c7-b02d-6f2471aaff80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bcd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd2733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
